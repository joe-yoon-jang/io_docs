---
title: LLM chapter1
layout: Study
---


# 1 LLM
- 2017년 google brain 팀은 transformer라는 진보된 인공지능 딥러닝 모델을 소개하였고 그 이후로 다양한 자연어처리(NLP: natural language processing) 작업을 다루는 데 표준이 되었다
- 이러한 트렌스포머 모델을 사용하는 다양한 모델을 의식하지 못한 채 이용하고 있었을 가능성이 높다
- 아래는 마소와 open ai가 협력하여 개발한 깃허브 코파일럿을 이용한 소스코드이다

```py
from transformers import pipeline
def classify_text(email):
  """
  Use facebook's BART model to classfy an email into "spam" or "not span" 
  Args:
    email str: the email to classify
  Returns:
    str: The classification of the email
  """
  # copilot 시작
    classifier = pipeline(
        "zero-shot-classification",
        model="facebook/bart-large-mnli",
        device=0,  # GPU 사용, CPU를 사용하려면 -1로 설정
        clean_up_tokenization_spaces=True,
    )
    labels = ["spam", "not spam"]
    hypothesis_template = "This email is {}."
    result = classifier(
        email, candidate_labels=labels, hypothesis_template=hypothesis_template
    )
    print(result)
    classification = result["labels"][0]
    return classification
  # copilot 끝
  
```

- 코파일럿 함수 정의와 몇몇 주석만 작성하여 동작을 수행하기 위한 모든 코드는 코파일럿이 작성

```py
classify_text("Get a free iPhone now!")
{'sequence': 'Get a free iPhone now!', 'labels': ['not spam', 'spam'], 'scores': [0.8335405588150024, 0.16645941138267517]}
```

- 호출 가능한 다음과 같은 파이썬 함수를 만들었다

## 1.1 LLM 이란

- LLM Large language model (대규모언어모델)
  - 대부분 트랜스포머 아키텍처에서 파생된 AI 모델로 사람의 언어 코드 등을 이해하고 생성하기 위해 설계되었다
  - 이러한 모델은 방대한 양의 텍스트 데이터로 학습되어 사람 언어의 복잡성과 뉘앙스를 포착 할수 있다
    - LLM 은 간단한 텍스트 분류부터 텍스트 생성에 이르기까지 언어관련 작업을 넓은 범위에서 높은 정확도로 유창하고 유려하게 수행할수 있다
      - LLM 은 의료산업 LLM 이 전자 의무기록 처리 임상시험 매칭 신약 발견 등에 사용되며
      - 금융분야 사기탐지, 금융뉴스의 감정 분석, 심지어 트레이딩 전략 등에 사용
      - 챗봇과 가상 어시스턴트를 통한 고객서비스 자동화에도 사용
  - LLM과 트랜스포머의 성공은 여러 아이디어의 결합
    - 어텐션, 전이학습, 신경망의 스케일링 등 메커니즘
    - 트랜스포머 아키텍처 자체는 매웅 니상적
      - 병렬화 와 확장 가능
      - 이전보다 훨씬 큰 데이터 셋과 훈련시간을 지원
      - 트랜스포머는 시퀀스 내 각 단어가 다른 모든 단어에 주의를 기울이게 하여 단어간의 장거리 종속성(long range dependency)과 문맥 관계를 포착할수 있게하는 특별한 종류의 어텐션 계산인 셀프 어텐션(self attention)을 사용
      - hugging face 와 같은 인기있는 llm 저장소가 등장하여 대중들에게 강력한 오픈 소스 모델에 대한 접근을 제공
    
### 1.1.1 LLM 정의
- NLP 하위 분야로 언어 모델링(Language model)이 있다    
  - 지정된 어휘(제한되고 알려진 토큰의 집합) 내의 토큰 시퀀스 가능성을 예측하기 위한 통계/딥러닝 모델의 생성을 포함
  - 일반 적으로 2종류의 언에 모델링 작업이 있다(자동인코딩/자기회귀)
    - 자동인코딩(Autoencoding language model): 알려진 어휘에서 문장의 어느 부분이든 누락된 단어를 채우도록 모델에 요청(ex: 표지판에서 __ 하지 않으면 벌금을 받게 될것입니다.(stop:95, yield: 5))
      - 손상된 버전의 입력 내용으로부터 기존 문장을 재구성하도록 훈련된다. 이러한 모델은 트랜스포머 모델의 인코더 부분에 해당하며 마스크 없이 전체 입력에 접근할수 있다. 자동 인코딩 모델은 전체문장의 양방향 포현을 생성. 텍스트 생성과 같은 다양한 작업에 파인튜닝 될수 있지만 쥬요 애플리케이션은 문장 분류 또는 토큰 분류. BERT가 있다.
    - 자기회귀(Autoregressive language model): 알려진 어휘에서 주어진 문장의 바로 다음에 가장 가능성 있는 토큰을 생성하도록 모델에 요청한다.(ex: if you don't (mind, want, have))
      - 문장에서 이전 토큰만을 기반으로 다음 토큰을 예측하도록 훈련된다. 이러한 모델은 트랜스포머 모델의 디코더 부분에 해당하며 어텐션 헤드가 앞서 온 토큰만 볼수 있도록 전체 문장에 마스크가 적용되어 있다. 텍스트 생성에 이상적이며 GPT가 있다.
  - 토큰: 의미를 가지는 가장 작은 단위로 문장이나 텍스트를 작은 단위로 나누어 생성. LLM의 기본입력. 토큰은 단어일수있지만 하위단어(subword)일수도 있다. n-gram이라는 용어는 n개의 연속된 시퀀스를 나타낸다
  - LLM은 자기회귀거나 자동 인코딩 또는 두가지의 조합이 도리수있는 언어 모델

### 1.1.2 LLM 주요 특징  
- 기존 트랜스포머 아키텍처는 시퀀스 투 시퀀스였다
- 시퀀스 투 시퀀스(sequence to sequence)(seq2seq)
  - 인코더/디코더 두가지 구성요소를 갖는다.
  - 인코더: 원시 텍스트를 받아들여 핵심 구성 요소로 분리하고 해당 구성 요소를 백터로 변환하는 업무를 담당하며 어텐션을 사용하여 텍스트의 맥략을 이해
  - 디코더: 수정죈 형식의 어텐션을 사용하여 다음에 올 최적의 토큰을 예측함으로써 텍스트를 생성하는데 뛰어나다

![seq2seq](../../assets/images/llm1-1.png)
- LLM의 주요 카테고리
  - GTP와 같은 자기회귀 모델 - 이전 토큰을 기반으로 문장의 다음 토큰을 예측. 주어진 맥락을 따라서 일관성 있는 텍스트를 생성하는데 효과 적
  - BERT와 같은 자동 인코딩 모델 - 입력 토큰 중 일부를 가리고 남아있는 토큰으로부터 그것을 예측하여 문맥을 양방향으로 이해하여 표현을 구축한다. 토큰간의 맥락적 관계를 빠르고 대규모로 포착하는데 능숙하여 텍스트 분류 작업에 이용하기에 좋다
  - T5와 같은 자기회귀와 자동인코딩의 조합

  ### 1.1.3 LLM 작동원리
  - LLM 이 어떻게 사전훈련되고 파인튜닝되는지에 따라 그저 괜찮은 성능의 LLM과 매우 정확한 LLM 사이의 차이가 만들어진다.

  - 사전훈련
    - 이름이 붙여진 거의 모든 LLM은 대량의 텍스트 데이터로 특정 언에 모델링 관련 작업에 대해 사전훈련(pre training) 되었다
    - MLM(masked language modeling) 마스크된 언어 모델링
      - 이스탄불은 방문하기에 훌륭한 [마스크]
    - NSP(next sentence prediction) 다음 문장 예측
      - A: 이스탄불은 방문하기에 훌륭한 도시입니다. B: 나는 거기 가봤습니다.
        - 문장 B는 A문장의 바로 다음에 왔습니까? 예 or 아니오
    - 사전 훈련 과정은 LLM 을 더 잘 훈련시키는 방법을 차자내고 도움이 되지 않는 방법을 단계적으로 제거함에 따라 발전
  - 전이학습(Transfer learning)
    - 머신러닝에서 한 작업에서 얻은 지식을 활용하여 다른 관련 작업의 성능을 샹상시키는 기술
    - LLM 에 대한 전이학습은 텍스트 데이터의 한 말뭉치에서 사전훈련된 LLM 을 가져온다
    - 실제 작업을 위해 작업 특정 데이터로 모델의 파라미터를 업데이트함으로써 모델을 파인튜닝하는 것을 포함한다
    - 기본 아이디어는 사전 훈련된 모델이 이미 특정 언어와 언어 내 단어 간의 관계에 대한 많은 정보를 학습했으며 이정보를 새로운 작업에서의 성능 향상 시작점으로 사용할수 있다는 것이다.
  - 파인튜닝(fine-tuning)
    - 사전 훈련된 LLM은 특정 작업을 위해 파인튜닝 될수있다.
    - LLM 을 작업에 특화된 상대적으로 작은 크기의 데이터셋에서 훈련시켜 특정 작업을 위한 파라미터를 조정하는것을 의미
    - 특정 도메인 및 작업에서의 성능을 크게 향상시키는 것으로 나타난다
  - 어텐션(attention)
    - 트렌스포머를 처음 소개한 논문의 제목은 어텐션만있으면됩니다.였다
    - 다양한 가중치를 입력의 다른 부분에 할당하는 딥러닝 모델에서 사용되는 메커니즘.
    - 중요한 정보를 우선시하고 강조할 수 있다
    - LLM 은 큰 말뭉치에서 사전 훈련되고 특정 작업을 위해 더 작은 데이터셋으로 파인튜닝된다. 그리고 트랜스포머가 언어 모델로서 좋은 역활을 하기위해서는 고도의 병렬처리가 가능해야하며 이로인해 더 빠른 훈련과 텍스트의 효율적인 처리가 가능해진다. 트랜스포머가 다른 딥러닝 아키텍처와 차별화되는 점은 토큰간의 장거리 의존성과 관계를 어텐션을 사용하여 포착할 수 있는 능력이다. 다시 말해 어텐션은 트랜스포머 기반 LLM 의 핵심 구성요소이며 훈련 과정과 대상 작업 사이의 정보를 효과적으로 유지하면서 긴 텍스트 부분을 쉽게 처리할수 있다.


